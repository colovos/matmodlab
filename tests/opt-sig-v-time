#!/usr/bin/env python
"""Provides a recipe for computing the maximum error between the expected
stress vs. time slope and the simulated.

"""
import os
import sys
import numpy as np

from utils.exo.exofile import ExodusIIReader

def main(*argv):
    vars_to_get = ("STRESS_XX", "STRESS_YY", "STRESS_ZZ")

    exof, aux = argv[:2]

    # read in baseline data
    auxhead = open(aux).readline().split()
    auxdat = np.loadtxt(aux, skiprows=1)
    I = np.array([auxhead.index(var) for var in vars_to_get], dtype=np.int)
    basesig = auxdat[:, I]
    basetime = auxdat[:, auxhead.index("TIME")]

    # read in output data
    exof = ExodusIIReader.new_from_exofile(exof)
    simtime = exof.get_all_times()
    simsig = np.transpose([exof.get_elem_var_time(var, 0) for var in vars_to_get])

    # do the comparison
    error = -1
    t0 = max(np.amin(basetime), np.amin(simtime))
    tf = min(np.amax(basetime), np.amax(simtime))
    n = basetime.shape[0]
    for idx in range(3):
        base = lambda x: np.interp(x, basetime, basesig[:, idx])
        comp = lambda y: np.interp(y, simtime, simsig[:, idx])
        rms = np.sqrt(np.mean([(base(t) - comp(t)) ** 2
                               for t in np.linspace(t0, tf, n)]))
        dnom = np.amax(np.abs(simsig[:, idx]))
        if dnom < 1.e-12: dnom = 1.
        error = max(rms / dnom, error)
        continue

    print error

    return 0

if __name__ == "__main__":
    sys.exit(main(*sys.argv[1:]))
